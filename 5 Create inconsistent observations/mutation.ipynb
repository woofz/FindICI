{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from typing import Dict\n",
    "import spacy  # For preprocessing\n",
    "from gensim.models import Word2Vec\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import seaborn as sns\n",
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from importlib import reload\n",
    "from copy import deepcopy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_train2 import remove_symbols_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tasks_top10_ast_tokenization2.pkl', 'rb') as input_file:\n",
    "    mapped10_ast_token = pickle.load(input_file)   \n",
    "mapped10_ast_token.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('top10_list.pkl', 'rb') as input_file:\n",
    "    top10_list = pickle.load(input_file)   \n",
    "top10_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m10 = deepcopy(mapped10_ast_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>shell</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m10 = m10[m10['mod_keys_found_string']=='shell'].reset_index(drop=True)\n",
    "m10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_string(col):\n",
    "    \n",
    "    a = ' '.join(col)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m10['token_names_one_string'] = m10['token_task_names'].apply(lambda x: remove_symbols_simple(x))\n",
    "m10['third_tokens'] = m10['third_tokens'].apply(lambda x: remove_symbols_simple(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m10['token_names_one_string'] = m10['token_names_one_string'].apply(lambda x: one_string(x))\n",
    "m10['third_tokens2_one_string'] = m10['third_tokens'].apply(lambda x: one_string(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m10['shell_used'] = m10['third_tokens'].apply(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Counter(\" \".join(m10[\"shell_used\"]).split()).most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m20 = deepcopy(m10[m10['shell_used'] == 'oc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m20['most_oc'] = m20['third_tokens'].apply(lambda x: x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(\" \".join(m20[\"most_oc\"]).split()).most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>command</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m30 = deepcopy(mapped10_ast_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m30 = m30[m30['mod_keys_found_string']=='command'].reset_index(drop=True)\n",
    "m30.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m30['token_names_one_string'] = m30['token_task_names'].apply(lambda x: remove_symbols_simple(x))\n",
    "m30['third_tokens'] = m30['third_tokens'].apply(lambda x: remove_symbols_simple(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m30['token_names_one_string'] = m30['token_names_one_string'].apply(lambda x: one_string(x))\n",
    "m30['third_tokens2_one_string'] = m30['third_tokens'].apply(lambda x: one_string(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m30['command_used'] = m30['third_tokens'].apply(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(\" \".join(m30[\"command_used\"]).split()).most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m40 = deepcopy(m30[m30['command_used'] == 'oc'])\n",
    "m40['most_oc'] = m40['third_tokens'].apply(lambda x: x[3])\n",
    "Counter(\" \".join(m40[\"most_oc\"]).split()).most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Set fact</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m50 = deepcopy(mapped10_ast_token)\n",
    "m50 = m50[m50['mod_keys_found_string']=='set_fact'].reset_index(drop=True)\n",
    "m50.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m50['token_names_one_string'] = m50['token_task_names'].apply(lambda x: remove_symbols_simple(x))\n",
    "m50['third_tokens'] = m50['third_tokens'].apply(lambda x: remove_symbols_simple(x))\n",
    "m50['token_names_one_string'] = m50['token_names_one_string'].apply(lambda x: one_string(x))\n",
    "m50['third_tokens2_one_string'] = m50['third_tokens'].apply(lambda x: one_string(x))\n",
    "m50['set_fact_used'] = m50['third_tokens'].apply(lambda x: x[2])\n",
    "Counter(\" \".join(m50[\"set_fact_used\"]).split()).most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m60 = deepcopy(m50[m50['set_fact_used'] == 'docker_device'])\n",
    "m60['most_set_fact'] = m60['third_tokens'].apply(lambda x: x[3])\n",
    "Counter(\" \".join(m60[\"most_set_fact\"]).split()).most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>template</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m50 = deepcopy(mapped10_ast_token)\n",
    "m50 = m50[m50['mod_keys_found_string']=='template'].reset_index(drop=True)\n",
    "m50.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m50['token_names_one_string'] = m50['token_task_names'].apply(lambda x: remove_symbols_simple(x))\n",
    "m50['third_tokens'] = m50['third_tokens'].apply(lambda x: remove_symbols_simple(x))\n",
    "m50['token_names_one_string'] = m50['token_names_one_string'].apply(lambda x: one_string(x))\n",
    "m50['third_tokens2_one_string'] = m50['third_tokens'].apply(lambda x: one_string(x))\n",
    "m50['template_used'] = m50['third_tokens'].apply(lambda x: x[2])\n",
    "Counter(\" \".join(m50[\"template_used\"]).split()).most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m60 = deepcopy(m50[m50['template_used'] == 'src'])\n",
    "m60['most_template'] = m60['third_tokens'].apply(lambda x: x[3])\n",
    "Counter(\" \".join(m60[\"most_template\"]).split()).most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>file</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m50 = deepcopy(mapped10_ast_token)\n",
    "m50 = m50[m50['mod_keys_found_string']=='file'].reset_index(drop=True)\n",
    "m50.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m50['token_names_one_string'] = m50['token_task_names'].apply(lambda x: remove_symbols_simple(x))\n",
    "m50['third_tokens'] = m50['third_tokens'].apply(lambda x: remove_symbols_simple(x))\n",
    "m50['token_names_one_string'] = m50['token_names_one_string'].apply(lambda x: one_string(x))\n",
    "m50['third_tokens2_one_string'] = m50['third_tokens'].apply(lambda x: one_string(x))\n",
    "m50['file_used'] = m50['third_tokens'].apply(lambda x: x[2])\n",
    "Counter(\" \".join(m50[\"file_used\"]).split()).most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m60 = deepcopy(m50[m50['file_used'] == 'path'])\n",
    "m60['most_file'] = m60['third_tokens'].apply(lambda x: x[3])\n",
    "Counter(\" \".join(m60[\"most_file\"]).split()).most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Gather facts</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m50 = deepcopy(mapped10_ast_token)\n",
    "m50 = m50[m50['mod_keys_found_string']=='gather_facts'].reset_index(drop=True)\n",
    "m50.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m50['token_names_one_string'] = m50['token_task_names'].apply(lambda x: remove_symbols_simple(x))\n",
    "m50['third_tokens'] = m50['third_tokens'].apply(lambda x: remove_symbols_simple(x))\n",
    "m50['token_names_one_string'] = m50['token_names_one_string'].apply(lambda x: one_string(x))\n",
    "m50['third_tokens2_one_string'] = m50['third_tokens'].apply(lambda x: one_string(x))\n",
    "m50['gather_facts_used'] = m50['third_tokens'].apply(lambda x: x[2])\n",
    "Counter(\" \".join(m50[\"gather_facts_used\"]).split()).most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m60 = deepcopy(m50[m50['gather_facts_used'] == 'False'])\n",
    "m60['most_gather_facts'] = m60['third_tokens'].apply(lambda x: x[3])\n",
    "Counter(\" \".join(m60[\"most_gather_facts\"]).split()).most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m10 = deepcopy(mapped10_ast_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m10['third_tokens'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Mutate methods</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_train2 import mutate_descriptions2,mutate_descriptions_old, remove_symbols_simple, remove_symbols, \\\n",
    "                         mutate_params, change_descriptions, rebuild_tokens, parse_string_to_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m20 = deepcopy(m10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m20 = m20.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m20['consistent'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m20.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m20['mod_keys_found'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m20['mod_keys_found_string'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_train2 import mutate_descriptions2,mutate_descriptions_old, remove_symbols_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m20 = m20[m20['mod_keys_found_string'] == 'service']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retokenization, because a lot of tasks were in the String format.   \n",
    "for index, row in m10.iterrows():\n",
    "    module = row['mod_keys_found_string']\n",
    "    if type(row['method_description'][module]) == str:\n",
    "        row['method_description'][module] = parse_string_to_tasks(row['method_description'][module])\n",
    "        \n",
    "m10_correct_tokens = rebuild_tokens(m10['method_description']).reset_index(drop=True)\n",
    "m10.drop(columns=['third_tokens'])\n",
    "m10['third_tokens'] = m10_correct_tokens['third_tokens']\n",
    "\n",
    "# Applying mutations\n",
    "for index, row in m20.iterrows():\n",
    "    module = row['mod_keys_found_string']\n",
    "    if type(row['method_description'][module]) == str:\n",
    "        row['method_description'][module] = parse_string_to_tasks(row['method_description'][module])\n",
    "    if row['method_description'][module] is None or isinstance(row['method_description'][module], bool):\n",
    "        row['method_description'][module] = dict()\n",
    "    mutations = change_descriptions(task=row['method_description'][module],\n",
    "                                    task_name=row['task_name'], module_used=module)\n",
    "    m20.loc[index, 'task_name'] = mutations['task_name']\n",
    "\n",
    "    \n",
    "# m20 Rebuild\n",
    "m20.reset_index(drop=True)\n",
    "m20_rebuild = rebuild_tokens(descriptions=m20['method_description'], task_names=m20['task_name']).reset_index(drop=True)\n",
    "print(m20_rebuild['token_task_names'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing Third Tokens\n",
    "\n",
    "print(m10['third_tokens'][0])\n",
    "m20.drop(columns=['third_tokens', 'token_task_names'])\n",
    "m20['third_tokens'] = m20_rebuild['third_tokens']\n",
    "m20['token_task_names'] = m20_rebuild['token_task_names']\n",
    "print(m20['third_tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m10['consistent'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m10.shape)\n",
    "m20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([m10,m20],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['token_task_names'] = merged['token_task_names'].apply(lambda x: remove_symbols_simple(x))\n",
    "merged['third_tokens'] = merged['third_tokens'].apply(lambda x: remove_symbols_simple(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = deepcopy(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m['token_task_names'] = m['token_task_names'].apply(lambda x: ['TaskName']+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = deepcopy(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_first_token_name(seq):\n",
    "    \n",
    "    seq[0] = 'TaskDescription'\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm['third_tokens'] = mm['third_tokens'].apply(lambda x: correct_first_token_name(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mm['third_tokens'][0])\n",
    "mm['token_task_names'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged2 = deepcopy(mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged2 = merged2.drop(columns=['task_complete'])\n",
    "# merged2 = merged2.drop(columns=['task_com'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged2['task_com'] = merged2['token_task_names']+merged2['third_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_first_token_name2(seq):\n",
    "    \n",
    "    seq.insert(0,'AnsibleTask')\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged2['task_complete'] = merged2['task_com'].apply(lambda x: correct_first_token_name2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged2['task_complete'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged2.to_pickle('mutated2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mutated2.pkl', 'rb') as input_file:\n",
    "    mutated = pickle.load(input_file)   \n",
    "mutated.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train, test, validation split</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = np.split(mutated.sample(frac=1), [int(.6*len(mutated)), int(.8*len(mutated))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(val.shape)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_pickle('train_set.pkl')\n",
    "# test.to_pickle('test_set.pkl')\n",
    "# val.to_pickle('val_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
